{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# see https://untitled-ai.slack.com/archives/CTA2NC0G7/p1618526841016100 for more details\n",
    "import os\n",
    "os.environ['MKL_THREADING_LAYER'] = 'GNU'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run tests in the shell of your experiment notebook by adding computronium to your python path.\n",
    "```\n",
    "cd standalone/ballworld\n",
    "PYTHONPATH=.:../../computronium pytest ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Read the bottom of this notebook at least once\n",
    "## The remainder of the notebook is all of the ways to run tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The best way to run tests after many changes\n",
    "\n",
    "--stepwise (can be abbreviated --sw) means:\n",
    "\n",
    "\"run until first failure, and when re-running, begin at the last failure\"\n",
    "\n",
    "-m selects tests via a \"mark\", ie, just that type of test\n",
    "\n",
    "Once the integration tests start succeeding, you may wish to run just the last line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pytest -s --stepwise --without-integration --without-slow-integration && \\\n",
    "pytest -s --stepwise -m \"integration_test\" && \\\n",
    "pytest -s --stepwise -m \"slow_integration_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The above, but without displaying test stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --stepwise --without-integration --without-slow-integration && \\\n",
    "pytest --stepwise -m \"integration_test\" && \\\n",
    "pytest --stepwise -m \"slow_integration_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# To run tests in parallel\n",
    "\n",
    "Add the `-n <NUM_WORKERS>` command when you use `pytest`.\n",
    "\n",
    "Be careful about running tests that will use the GPU with multiple workers. Tests marked with `@slow_integration` or `@integration`\n",
    "can run tests on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pytest --stepwise -n 64 --without-integration --without-slow-integration --durations=0 && \\\n",
    "pytest --stepwise -m \"integration_test\" --durations=0 && \\\n",
    "pytest --stepwise -m \"slow_integration_test\" --durations=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To run a specific test\n",
    "\n",
    "The matching is done by string--can be module name, test name, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest -s -k \"test_check_stable\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To debug in pycharm\n",
    "\n",
    "This flag can be added to any other test.\n",
    "\n",
    "In order for this to work, you need to start your pycharm remote debug setup **before** you run this command!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest -s -k \"test_check_stable\" --pycharm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To debug in ipdb\n",
    "\n",
    "This flag can be added to any other test.\n",
    "\n",
    "This is particularly useful if you're lazy. I'm not sure if it will work well from the jupyter notebook--you may need to run this from the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest -s -k \"test_check_stable\" --pdbcls=IPython.terminal.debugger:TerminalPdb --pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad\n",
    "\n",
    "A place for you to run whatever command you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important information about tests\n",
    "\n",
    "## Properly setting up paths and env\n",
    "While the notebook contains the correct python path, you'll need to run this in your terminal to set it correctly:\n",
    "```\n",
    "export PYTHONPATH=.\n",
    "```\n",
    "You'll also obviously want to change directories to be in your 'current' directory.\n",
    "\n",
    "## How to create new tests\n",
    "\n",
    "Simply define any function with a name that starts with `test_` in a python file that either starts with `test_` or ends with `_test.py`.\n",
    "\n",
    "## Naming conventions\n",
    "\n",
    "Tests of a single module named `foo.py` should be located in `foo_test.py`.\n",
    "\n",
    "Higher level tests of multiple modules together should be placed in a file called `test_whatever.py`, where \"whatever\" should describe what you're trying to test.\n",
    "\n",
    "## How to use pytest fixtures\n",
    "\n",
    "Seriously, please go read this (yes the whole thing): https://docs.pytest.org/en/latest/fixture.html\n",
    "\n",
    "## How we prevent pytest fixtures from being totally unmaintainable\n",
    "\n",
    "pytest fixtures are really great in a lot of ways. However, one way in which they are not great is that they are kind of magical--they rely on the name of a parameter to a function in order to dynamically figure out which fixture to use, with all sorts of insane possibilities.\n",
    "\n",
    "We restrict ourselves (by convention) from using some of the crazy name overriding that is possible with pytest.\n",
    "\n",
    "Additionally, we've created our own little wrappers that make it possible to be explicit (per the zen of python) about which fixtures we are using.\n",
    "\n",
    "Use our **`@fixture`** annotation rather than the pytest one. Note that in order for it to work, you'll want to make the fixture name have an extra `_` at the end--this prevents it from shadowing the name of the variable in the function (which should **not** have that trailing `_`)\n",
    "\n",
    "The **`@use`** annotation indicates which fixtures should be used in the test. In the future, we will likely be able to autogenerate this annotation. For now, you need to keep it in-sync with the function definition.\n",
    "\n",
    "## Test style\n",
    "\n",
    "Please try to keep the duplication of code between tests as low as possible by using pytest fixtures, ideally defined locally, to specify the common resources that should be created and provided to the tests.\n",
    "\n",
    "\n",
    "## There are 3 types of tests\n",
    "\n",
    "1. **unit**: These should run extremely quickly (< 0.5 seconds). They should only really test a single module. Their file names should end in `_test.py`, and their names should be synchronized with the name of the file that they are testing. They should not use any external resources (processes, sockets, etc). Temp directories are ok.\n",
    "\n",
    "2. **integration**: These should run relatively quickly (0.1 second - 10 seconds). They can use any resources (but obviously like any test need to clean up after themselves and be safe from a concurrency perspective). They can also involve more than a single module. This is a good fit for something like \"check that godot can create some small number of images\".\n",
    "\n",
    "3. **slow integration**: These are much like the integration tests, but are allowed to take up to 100 seconds. This is a good fit for something like \"run many godot processes in parallel and soak test some particular thing\". Please try to not to make these. There's no guarantee that they will be run very often because they're so annoyingly slow.\n",
    "\n",
    "In order to define which type of test a given test is, use the `@integration_test` and `@slow_integration_test` markers from `common.testing_utils`. If unmarked, the test is assumed to be a unit test.\n",
    "\n",
    "## Notes on output formatting\n",
    "\n",
    "tqdm works well in notebooks and in the terminal, but if running pytest via a notebook there is no way for it to properly update the output progress bars due to the way that input is captured by pytest and sent to the notebook (because it is not a tty). Thus it is disabled for subprocesses, which would otherwise cause an insane amount of log spam.\n",
    "\n",
    "## mypy speed\n",
    "\n",
    "You might think \"ugh mypy is so slow\", but it actually does a good job of caching, so the *second* time you run it, it will be quite fast. That's why it's a unit test. However, for it to actually be fast, it needs to by run from the same directory, as that is where it creates its `.mypy_cache` directory.\n",
    "\n",
    "## About this notebook\n",
    "\n",
    "Because this notebook is synced from your local environment to the notebook machine, but is **not** synced back (unless you run an explicit scp command yourself), it is safe to edit this notebook and put in concrete values for whatever test you want to run, and even hit save. Note that when you next sync your local copy of this to the notebook machine and reload your browser, your changes will be gone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}